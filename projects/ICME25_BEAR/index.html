<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>Comming Soon!</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>·
&lt;a href="https://hu-cheng-yang.github.io/projects/ICME25_BEAR"&gt;Project Page&lt;/a&gt;
</code></pre></div></div> <p>&lt;/p&gt;</p> <h2 id="abstract">Abstract</h2> <p>Behavior recognition is an important task in video representation learning. An essential aspect pertains to effective feature learning conducive to behavior recognition. Recently, researchers have started to study fine-grained behavior recognition, which provides similar behaviors and encourages the model to concern with more details of behaviors with effective features for distinction. However, previous fine-grained behaviors limited themselves to controlling partial information to be similar, leading to an unfair and not comprehensive evaluation of existing works. In this work, we develop a new video fine-grained behavior dataset, named BEAR, which provides fine-grained (i.e. similar) behaviors that uniquely focus on two primary factors defining behavior: Environment and Action. It includes two fine-grained behavior protocols including Fine-grained Behavior with Similar Environments and Fine-grained Behavior with Similar Actions as well as multiple sub-protocols as different scenarios. Furthermore, with this new dataset, we conduct multiple experiments with different behavior recognition models. Our research primarily explores the impact of input modality, a critical element in studying the environmental and action-based aspects of behavior recognition. Our experimental results yield intriguing insights that have substantial implications for further research endeavors.</p> <h2 id="data-access">Data Access</h2> <p>We will release the dataset and protocols as soon as possible.</p> <h2 id="citing-our-work">Citing Our Work</h2> <p>If you find our work is useful in your reasearch, please consider citing:</p> <div class="language-bib highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hu2024domain</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{Domain-Hallucinated Updating for Multi-Domain Face Anti-spoofing}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Hu, Chengyang and Zhang, Ke-Yue and Yao, Taiping and Liu, Shice and Ding, Shouhong and Tan, Xin and Ma, Lizhuang}</span><span class="p">,</span>
  <span class="na">booktitle</span><span class="p">=</span><span class="s">{Proceedings of the AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">volume</span><span class="p">=</span><span class="s">{38}</span><span class="p">,</span>
  <span class="na">number</span><span class="p">=</span><span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span><span class="p">=</span><span class="s">{2193--2201}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span>
<span class="p">}</span>
</code></pre></div></div> <h2 id="acknowledgments">Acknowledgments</h2> <p>Our work is based on the following opensource project. We thank their authors for making the source code publically available.</p> <ul> <li> <a href="https://github.com/open-mmlab/mmaction2" rel="external nofollow noopener" target="_blank">mmaction</a> –&gt;</li> </ul> </body></html>